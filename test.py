import numpy as np
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler

# Stock Data (Replace this with real stock data)
stock_data = np.array([50600.0, 51400.0, 51811.0, 50050.0, 51455.0, 51487.3, 52538.0, 49990.0, 50388.0, 50200.0, 49501.0, 49105.0, 49998.5, 50960.0, 50374.0, 50000.0, 50000.0, 50000.0, 50110.0, 50999.0, 50997.3, 50997.0, 51000.0, 52999.0, 52000.0, 51998.0, 51000.0, 51082.5, 52125.0, 54100.0, 53732.0, 55920.0, 54800.0, 56181.6, 54000.0, 56668.0, 56805.8, 52999.0, 48794.7, 48000.0, 47328.0, 45950.0, 47799.0, 46100.0, 45200.0, 45499.9, 45000.0, 45359.4, 45350.0, 46100.0, 44982.0, 45900.0, 46000.0, 46190.0, 45287.0, 45300.0, 45000.0, 45070.0, 45100.0, 45000.0, 45350.0, 44500.0, 45828.0, 44501.0, 45933.6, 45943.8, 46900.0, 46900.0, 46320.0, 43643.0, 49250.0, 47614.7, 43286.1, 39500.0, 39550.0, 39800.0, 40399.9, 40200.0, 39780.0, 39000.0, 40290.0, 39882.0, 39000.0, 38710.0, 38711.0, 39500.0, 39500.0, 40199.0, 39780.0, 39005.0, 39641.0, 40450.0, 40450.0, 39550.1, 40300.0, 40595.9, 40000.0, 40000.0, 40100.0, 40600.0, 40600.0, 40000.0, 40400.0, 39700.0, 39899.9, 40500.0, 39900.0, 39900.0, 39898.0, 40800.0, 40494.0, 39780.0, 39500.0, 39500.0, 39900.0, 41100.0, 42300.0, 39500.0, 40443.0, 40000.0, 39788.9, 39086.0, 38350.0, 38500.0, 39183.4, 39983.0, 40000.0, 40400.0, 40474.0, 41300.0, 40800.0, 41500.0, 40200.0, 41588.4, 41604.6, 41604.6, 40000.0, 40087.0, 39301.0, 40600.0, 39800.0, 39684.1, 40000.0, 41000.0, 40200.0, 39882.0, 40300.0, 39525.0, 39185.0, 39984.0, 40000.0, 39781.0, 39200.0, 41297.2, 43000.0, 42630.0, 43500.0, 43152.4, 44033.0, 42350.0, 39740.0, 39400.0, 38800.0, 38199.0, 38862.0, 39000.0, 39250.0, 39882.0, 39200.0, 39500.0, 42328.0, 41499.0, 41485.0, 41245.0, 41043.7, 42736.0, 45151.0, 44209.0, 40902.0, 40400.0, 39700.0, 41000.0, 37944.0, 37490.0, 36900.0, 36200.0, 36190.0, 36000.0, 36000.0, 36250.3, 36990.0, 37000.0, 36350.0, 36564.9, 38000.0, 36600.0, 35555.0, 35300.0, 35300.0, 35222.0, 35500.0, 35300.0, 35700.0, 35750.0, 35444.0, 36440.0, 36400.0, 36900.0, 37222.3, 35590.0, 35500.0, 35400.0, 35490.0, 35001.0, 35500.0, 35500.0, 34830.0, 35750.0, 35700.0, 35900.0, 35900.0, 36400.0, 36399.9, 36605.7, 36000.0, 36560.0, 36615.0, 36000.0, 36680.0, 37500.0, 37944.0, 38000.0, 38790.0, 39349.0, 37850.0, 36975.0, 37500.5, 37420.0, 36650.0, 36900.0, 36700.0, 37800.0, 37100.0, 36700.0, 37700.0, 39500.0, 39500.0, 38754.0, 38430.0, 39290.0, 39930.0, 36499.9, 33657.9, 32998.0, 32700.0, 33226.0, 32574.9, 32595.0, 31915.0, 32320.0, 32262.8, 32130.0, 32700.0, 32664.0, 33359.0, 33199.0, 33000.0, 33333.3, 33952.0, 33333.0, 31927.0, 32808.0, 32500.0, 33238.6, 33900.0, 37838.0, 35322.0, 33342.7, 31000.0, 31925.0, 31742.0, 31400.0, 31925.0, 31140.0, 32758.5, 33427.0, 33869.0, 30800.0, 29370.0, 26700.0, 25400.0, 25600.0, 25300.0, 25350.0, 24990.0, 25195.0, 24895.0, 24480.0, 24241.0, 24750.0, 23400.0, 23664.0, 23200.0, 22542.0, 22100.0, 23088.8, 24225.0, 23750.0, 23500.0, 23127.0, 23024.0, 23200.0, 22902.0, 23052.0, 23529.8, 25200.0, 25000.0, 23868.0, 22748.0, 22083.0, 21650.0, 21503.0, 21550.0, 21500.0, 21650.0, 21700.0, 21550.0, 21500.0, 21726.0, 21300.0, 21320.0, 20910.0, 20500.0, 20210.0, 20145.0, 20145.0, 19900.0, 19951.0, 20300.0, 20000.0, 20000.0, 20000.0, 20240.0, 20229.0, 20349.0, 20100.0, 20300.0, 20346.0, 20700.0, 21560.0, 22000.0, 20269.0, 19844.5, 19000.0, 18800.0, 18500.0, 18450.0, 18500.0, 18450.0, 18450.0, 18300.0, 18500.0, 18400.0, 18450.0, 18450.0, 18350.0, 18559.0, 18150.0, 18500.0, 18995.0, 19050.0, 18300.0, 18717.0, 18350.0, 18499.0, 18900.0, 18768.0, 18400.0, 18725.0, 18865.0, 18550.0, 18797.0, 18480.0, 18940.0, 18965.0, 18599.0, 19050.0, 18500.0, 18600.0, 18870.0, 18500.0, 18415.0, 18360.0, 18555.0, 18600.0, 18972.0, 18600.0, 18600.0, 19306.0, 20200.0, 19998.8, 19600.0, 19200.0, 19200.0, 19480.0, 19490.0, 19690.0, 20300.0, 20499.0, 19999.0, 18364.0, 18300.0, 18000.0, 18100.0, 18150.0, 18120.0, 18360.0, 18360.0, 18000.0, 17900.0, 18091.0, 17800.0, 17998.0, 17646.0, 17400.0, 18197.8, 17750.0, 17701.0, 17688.0, 18400.0, 18421.0, 18100.0, 18100.0, 18399.9, 18200.0, 18200.0, 18400.0, 18225.0, 18400.0, 18550.0, 18784.0, 18800.0, 18880.0, 18575.0, 18500.0, 18600.0, 19000.0, 18720.0, 18500.0, 18690.0, 18600.0, 19660.5, 19275.4, 18897.5, 20250.0, 20600.0, 19750.0, 19793.1, 17993.8, 18000.0, 17900.0, 18000.0, 17999.8, 18100.0, 18000.0, 17650.0, 18000.0, 18000.0, 18666.0, 18300.0, 18500.0, 18950.0, 18799.0, 18551.0, 18360.0, 18180.0, 18005.0, 18005.0, 18180.0, 18300.0, 18300.0, 18500.0, 18335.0, 18700.0, 18400.0, 18400.0, 18749.0, 18200.0, 18106.0, 17950.0, 17910.0, 18100.0, 18000.0, 18100.0, 18461.0, 18100.0, 18100.0, 18499.0, 18499.0, 18129.6, 18499.0, 18227.1, 18600.0, 18620.0, 19000.0, 19089.9, 18717.0, 18350.0, 18490.0, 18600.0, 18750.0, 18600.0, 18600.0, 18600.0, 18950.0, 18950.0, 19299.0, 19000.0, 18700.0, 18700.0, 18700.0, 18800.0, 19140.0, 19147.0, 18772.0, 19398.0, 19398.0, 19377.0, 19100.0, 18718.0, 19100.0, 19002.0, 19379.9, 19200.0, 19105.0, 19500.0, 19800.0, 19500.0, 19449.0, 19173.9, 18600.0, 18463.0, 18500.0, 18500.0, 18700.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19200.0, 19100.0, 19382.0, 19050.0, 19100.0, 19050.0, 19000.0, 19125.0, 19500.0, 19125.0, 19100.0, 19600.0, 19400.0, 19500.0, 19500.0, 19450.0, 19162.0, 19533.0, 19150.1, 19075.0, 19409.0, 19400.0, 19200.0, 19176.0, 18900.0, 19480.0, 19110.0, 19150.0, 19000.0, 19227.0, 18850.0, 18900.0, 19000.0, 19312.0, 18564.0, 18250.0, 18400.0, 18768.0, 18550.0, 18900.0, 19000.0, 18300.0, 18900.0, 19000.0, 19295.0, 19005.0, 19100.0, 19599.0, 19700.0, 20000.0, 20277.6, 19880.0, 19499.0, 19558.0, 19159.0, 19600.0, 19736.0, 19012.0, 19400.0, 19500.0, 19142.9, 19533.5, 19000.0, 18700.0, 18798.0, 18999.0, 19355.0, 20101.0, 19686.0, 19000.0, 18750.0, 19100.0, 19176.0, 19100.0, 19650.0, 19650.0, 19380.0, 19500.0, 19074.0, 19200.0, 19700.0, 19650.0, 20022.4, 19630.0, 20300.0, 20100.0, 20800.0, 20800.0, 20699.0, 19500.0, 19700.0, 19466.0, 19850.0, 19800.0, 19600.0, 19620.0, 20000.0, 20000.0, 20250.0, 20500.0, 20344.0, 21200.0, 19951.2, 20400.0, 20380.0, 19982.0, 19990.0, 20000.0, 20298.0, 20000.0, 20375.0, 20493.0, 19698.1, 20500.0, 20449.8, 20300.0, 20388.0, 20298.0, 20349.8, 20185.0, 19800.0, 19441.0, 19790.0, 19900.0, 19980.0, 19800.0, 19351.0, 19700.0, 19700.0, 19350.0, 19800.0, 20000.0, 20090.0, 20000.0, 20500.0, 20130.0, 20300.0, 19997.0, 19300.0, 19604.0, 20094.0, 20397.0, 19998.0, 20000.0, 19412.0, 19310.0, 19320.0, 20050.0, 19810.0, 20000.0, 20200.0, 20000.0, 20300.0, 20300.0, 20800.0, 20400.0, 20552.0, 20552.0, 20400.0, 21130.0, 20899.0, 21100.0, 21650.0, 21350.0, 23180.0, 21505.0, 19550.0, 19079.0, 19079.0, 19100.0, 18903.0, 19279.0, 19430.0, 19062.0, 19600.0, 19476.0, 19095.0, 19433.0, 19440.0, 19500.0, 19294.0, 19300.0, 19300.0, 19500.0, 20049.0, 19897.0, 19601.0, 20182.0, 19790.0, 20190.0, 20300.0, 20200.0, 20400.0, 20199.0, 19990.0, 19869.0, 19210.0, 19600.0, 19727.0, 19000.0, 19700.0, 19584.0, 19200.0, 19000.0, 19184.0, 18450.0, 18450.0, 19101.0, 18728.0, 19121.0, 19759.0, 19610.0, 19750.0, 19870.0, 20296.0, 19899.0, 19690.0, 19700.0, 19600.0, 19500.0, 20000.0, 20149.0, 20000.0, 20000.0, 20430.0, 19800.0, 19700.0, 20200.0, 20245.0, 20250.0, 20250.0, 20080.0, 20160.0, 20440.0, 20500.0, 20550.0, 20310.0, 20800.0, 20502.0, 20081.0, 19688.0, 19550.0, 19593.0, 19561.0, 19100.0, 19000.0, 18890.0, 19200.0, 19600.0, 19506.0, 18850.0, 18940.0, 18800.0, 19160.0, 19551.0, 19559.0, 18990.0, 19074.0, 19500.0, 19600.0, 19580.0, 19900.0, 20200.0, 19325.0, 19999.0, 20592.0, 20189.0, 19555.0, 20912.0, 21184.0, 21300.0, 21800.0, 23378.0, 22950.0, 21900.0, 20287.0, 21000.0, 22378.0, 20344.0, 18500.0, 18500.0, 18460.0, 18000.0, 17901.0, 18006.0, 18650.0, 18565.0, 18360.0, 18000.0, 18100.0, 18101.0, 18000.0, 17515.0, 17802.0, 17560.0, 18129.0, 18717.0, 18350.0, 18000.0, 18252.0, 18100.0, 18100.0, 18050.0, 18200.0, 18200.0, 18259.0, 18200.0, 19074.0, 19800.0, 19800.0, 19890.0, 19800.0, 19889.0, 19502.0, 20000.0, 20250.0, 19890.0, 19247.0, 18870.0, 18717.0, 19000.0, 18800.0, 18492.0, 18500.0, 18300.0, 18300.0, 18620.0, 19000.0, 19000.0, 19000.0, 19210.0, 19210.0, 19210.0, 19210.0, 19210.0, 19600.0, 19600.0, 19966.0, 19975.0, 18800.0, 19111.0, 19500.0, 19569.0, 19186.0, 19193.0, 19584.0, 19881.0, 21122.0, 21992.0, 22440.0, 22865.0, 23345.0, 21596.0, 19633.0, 17849.0, 18213.0, 18590.0, 18963.0, 19350.0, 18906.0, 18536.0, 18914.0, 18914.0, 19300.0, 19300.0, 19100.0, 19050.0, 19050.0, 19000.0, 18941.0, 18571.0, 17535.0, 17892.0, 17900.0, 18633.0, 19015.0, 19401.0, 19796.0, 20200.0, 20185.0, 18350.0, 18621.0, 19000.0, 19200.0, 19400.0, 19400.0, 19400.0, 19500.0, 19208.0, 20000.0, 20000.0, 19792.0, 20195.0, 20000.0, 20000.0, 20400.0, 20250.0, 20250.0, 20035.0, 19649.0, 19264.0, 19264.0, 18517.0, 18894.0, 19279.0, 19672.0, 19287.0, 19680.0, 19304.0, 19700.0, 19700.0, 19590.0, 19210.0, 19600.0, 20000.0, 19630.0, 19250.0, 19600.0, 20400.0, 20000.0, 20000.0, 20000.0, 19796.0, 20200.0, 20200.0, 20090.0, 20500.0, 20282.0, 20282.0, 20695.0, 21117.0, 20703.0, 20298.0, 19900.0, 19743.0, 18977.0, 18613.0, 18992.0, 19306.0, 19700.0, 19379.0, 19686.0, 19300.0, 19000.0, 19380.0, 19000.0, 19390.0, 19380.0, 19000.0, 19105.0, 19492.0, 19492.0, 19500.0, 20413.0, 19621.0, 18863.0, 19247.0, 19247.0, 18870.0, 18166.0, 18536.0, 18914.0, 19300.0, 19657.0, 19657.0, 20475.0, 20892.0, 20892.0, 21109.0, 23402.0, 24376.0, 23899.0, 23431.0, 22522.0, 22081.0, 22090.0, 22540.0, 23000.0, 23818.0, 24304.0, 24799.0, 24673.0, 24190.0, 24200.0, 23600.0, 24000.0, 23031.0, 23501.0, 23965.0, 23501.0, 23975.0, 22599.0, 23530.0, 24300.0, 24500.0, 25000.0, 25450.0, 25500.0, 25800.0, 25921.0, 26450.0, 26000.0, 25900.0, 25950.0, 26479.0, 27019.0, 27570.0, 26500.0, 26500.0, 25472.0, 25989.0, 25480.0, 26000.0, 26500.0, 26200.0, 26692.0, 26169.0, 25666.0, 26200.0, 26500.0, 25637.0, 24643.0, 25647.0, 25145.0, 25667.0, 25687.0, 26211.0, 24701.0, 25163.0, 25676.0, 26200.0, 26264.0, 26900.0, 26700.0, 27244.0, 27800.0, 28000.0, 27966.0, 27418.0, 27977.0, 27988.0, 27440.0, 28000.0, 0.0, 28311.0, 27756.0, 27756.0, 28322.0, 29300.0, 29070.0, 28500.0, 28200.0, 28224.0, 28800.0, 28600.0, 29000.0, 28800.0, 28700.0, 29000.0, 28700.0, 28988.0, 29400.0, 30000.0, 29001.0, 29001.0, 27990.0, 28560.0, 28240.0, 28812.0, 30000.0, 30600.0, 30000.0, 30001.0, 30088.0, 30088.0, 30702.0, 30200.0, 30000.0, 29950.0, 29950.0, 30001.0, 30001.0, 30000.0, 30000.0, 29886.0, 29300.0, 29000.0, 29000.0, 29201.0, 29200.0, 29800.0, 30069.0, 29000.0, 29000.0, 29400.0, 30000.0, 30000.0, 30170.0, 29580.0, 29580.0, 29580.0, 29580.0, 29000.0, 29000.0, 29000.0, 30000.0, 29976.0, 30587.0, 30000.0, 30200.0, 29580.0, 29000.0, 29000.0, 29000.0, 28651.0, 28090.0, 27540.0, 27000.0, 27500.0, 27550.0, 27852.0, 28420.0, 29000.0, 29498.0, 30100.0, 30260.0, 30253.0, 30870.0, 31500.0, 30900.0, 32144.0, 33086.0, 33761.0, 34450.0, 33777.0, 33115.0, 33115.0, 33780.0, 32480.0, 33130.0, 33800.0, 33836.0, 35250.0, 35021.0, 35111.0, 35011.0, 35000.0, 35000.0, 35192.0, 35000.0, 35700.0, 35190.0, 34500.0, 35200.0, 35200.0, 35000.0, 35000.0, 34500.0, 35001.0, 34501.0, 35000.0, 34500.0, 34400.0, 34681.0, 34008.0, 31421.0, 30201.0, 31437.0, 32078.0, 32733.0, 33400.0, 34000.0, 34000.0, 34000.0, 34000.0, 34950.0, 34575.0, 35280.0, 36000.0, 35370.0, 34678.0, 33333.0, 32252.0, 32252.0, 32252.0, 31620.0, 31620.0, 31212.0, 29600.0, 29580.0, 29000.0, 28505.0, 28500.0, 28500.0, 28900.0, 28500.0, 28128.0, 27577.0, 27037.0, 26507.0, 25988.0, 25479.0, 24491.0, 24990.0, 25500.0, 25186.0, 25700.0, 26100.0, 25700.0, 25700.0, 25725.0, 26250.0, 25800.0, 25980.0, 26000.0, 26000.0, 26520.0, 26000.0, 26000.0, 28411.0, 28990.0, 28910.0, 30380.0, 31000.0, 31212.0, 30600.0, 30000.0, 30000.0, 30000.0, 28700.0, 28153.0, 26010.0, 25500.0, 25000.0, 25000.0, 23700.0, 22431.0, 21992.0, 21400.0, 22000.0, 21436.0, 20604.0, 20300.0, 20280.0, 20000.0, 20000.0, 20200.0, 19880.0, 19880.0, 19880.0, 19880.0, 19492.0, 19110.0, 19801.0, 19413.0, 18295.0, 17937.0, 18666.0, 18300.0, 17998.0, 17646.0, 17300.0, 17400.0, 17200.0, 17100.0, 16930.0, 16600.0, 16300.0, 16015.0, 16015.0, 15700.0, 15150.0, 15190.0, 15500.0, 15600.0, 15400.0, 15400.0, 15700.0, 15700.0, 15500.0, 15123.0, 15744.0, 16065.0, 15300.0, 15200.0, 15100.0, 15150.0, 15100.0, 15000.0, 15229.0, 14504.0, 14800.0, 15054.0, 15361.0, 14630.0, 15225.0, 14897.0, 15200.0, 15184.0, 14596.0, 14310.0, 14310.0, 14872.0, 14602.0], dtype=float)

# Normalize data
scaler = MinMaxScaler(feature_range=(0, 1))
stock_data_normalized = scaler.fit_transform(stock_data.reshape(-1, 1)).flatten()

# Sequence length
sequence_length = 3

# Prepare data sequences
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

train_X, train_y = create_sequences(stock_data_normalized, sequence_length)

# Xavier initialization
def initWeights(input_size, output_size):
    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))

# Activation functions
def sigmoid(x, derivative=False):
    x = np.clip(x, -500, 500)
    if derivative:
        return x * (1 - x)
    return 1 / (1 + np.exp(-x))

def tanh(x, derivative=False):
    if derivative:
        return 1 - x ** 2
    return np.tanh(x)

# LSTM class
# LSTM class
class LSTM:
    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):
        self.learning_rate = learning_rate
        self.hidden_size = hidden_size
        self.num_epochs = num_epochs

        # Initialize weights for gates
        self.wf = initWeights(input_size + hidden_size, hidden_size)  # shape (hidden_size, input_size + hidden_size)
        self.bf = np.zeros((hidden_size, 1))

        self.wi = initWeights(input_size + hidden_size, hidden_size)  # shape (hidden_size, input_size + hidden_size)
        self.bi = np.zeros((hidden_size, 1))

        self.wc = initWeights(input_size + hidden_size, hidden_size)  # shape (hidden_size, input_size + hidden_size)
        self.bc = np.zeros((hidden_size, 1))

        self.wo = initWeights(input_size + hidden_size, hidden_size)  # shape (hidden_size, input_size + hidden_size)
        self.bo = np.zeros((hidden_size, 1))

        # Output weight
        self.wy = initWeights(hidden_size, output_size)  # shape (output_size, hidden_size)
        self.by = np.zeros((output_size, 1))


    def reset(self):
        self.hidden_states = {-1: np.zeros((self.hidden_size, 1))}
        self.cell_states = {-1: np.zeros((self.hidden_size, 1))}
        self.outputs = {}

    def forward(self, inputs):
        self.reset()
        outputs = []

        for t in range(len(inputs)):
            # Take a single value at time step t (not the entire sequence)
            input_t = inputs[t].reshape(1, 1)  # Reshape to (1, 1), representing (input_size, 1)

            print(f"Shape of input_t: {input_t.shape}")
            print(f"Shape of hidden_state: {self.hidden_states[t - 1].shape}")

            # Concatenate hidden state and input at time t
            concat_input = np.concatenate((self.hidden_states[t - 1], input_t), axis=0)
            print(f"Shape of concat_input: {concat_input.shape}")

            # Apply gates
            ft = sigmoid(np.dot(self.wf, concat_input) + self.bf)
            it = sigmoid(np.dot(self.wi, concat_input) + self.bi)
            ct_tilde = tanh(np.dot(self.wc, concat_input) + self.bc)
            ot = sigmoid(np.dot(self.wo, concat_input) + self.bo)

            # Update cell state and hidden state
            self.cell_states[t] = ft * self.cell_states[t - 1] + it * ct_tilde
            self.hidden_states[t] = ot * tanh(self.cell_states[t])

            # Output layer
            output = np.dot(self.wy, self.hidden_states[t]) + self.by
            outputs.append(output)

        return outputs



    def backward(self, errors, inputs):
        # Backpropagation steps omitted for clarity (same as before)
        pass

    def train(self, X, y):
        for epoch in tqdm(range(self.num_epochs)):
            # Run the forward pass to get predictions
            predictions = self.forward(X)

            # y is already a scalar (single value), so no need to reshape or loop through it
            error = y - predictions[-1]  # Calculate the error with the last prediction

            # Run the backward pass to adjust the weights (if implemented)
            self.backward([error], X)  # Backward expects a list of errors


    def predict(self, X):
        return self.forward(X)

# Training the LSTM
input_size = 1  # Fix input size to 1, as each time step provides one input value
hidden_size = 25
output_size = 1
num_epochs = 500
learning_rate = 0.01

lstm = LSTM(input_size, hidden_size, output_size, num_epochs, learning_rate)

# Reshape input for LSTM
print(train_X.shape)
train_X_reshaped = train_X.reshape((train_X.shape[0], sequence_length, 1))
for i in range(len(train_X_reshaped)):
    lstm.train(train_X_reshaped[i], train_y[i])

# Predict next 7 days
def predict_next_days(model, data, days):
    predictions = []
    current_seq = data[-sequence_length:]
    
    for _ in range(days):
        current_seq_reshaped = current_seq.reshape((sequence_length, 1))  # Reshape for LSTM
        next_pred = model.predict(current_seq_reshaped)[-1]
        predictions.append(next_pred)
        current_seq = np.append(current_seq[1:], next_pred)  # Shift window for next prediction
        
    return predictions

predictions = predict_next_days(lstm, stock_data_normalized, 7)
predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))

print(predictions)
